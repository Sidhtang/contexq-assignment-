{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDOK4duho_Ns"
      },
      "outputs": [],
      "source": [
        "AIzaSyCrWhqc8Jr_sNHm2pIIIUyNy6bD8nJV8tI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade feedparser pandas numpy matplotlib beautifulsoup4 networkx python-dotenv google-generativeai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVO5ADZUrj3T",
        "outputId": "d7e1f0eb-1fb3-4279-e8f3-223f575b9523"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m695.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m887.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=edf53e6fd30ec9243f6992ed5abe9899ebb0a129a6b9aaddc956db09080e546a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, python-dotenv, numpy, feedparser, pandas, matplotlib\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed feedparser-6.0.11 matplotlib-3.10.3 numpy-2.2.5 pandas-2.2.3 python-dotenv-1.1.0 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPSBlAKcpC3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import json\n",
        "import networkx as nx\n",
        "from networkx.algorithms import community\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure Gemini API with a hardcoded API key\n",
        "GEMINI_API_KEY = \"\"  # Replace with your actual API key\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Model configuration - using Gemini 2.0 Flash\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "# Define RSS feeds\n",
        "rss_feeds = {\n",
        "    \"Reuters Business\": \"http://feeds.reuters.com/reuters/businessNews\",\n",
        "    \"BBC World\": \"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
        "    \"Al Jazeera\": \"https://www.aljazeera.com/xml/rss/all.xml\"\n",
        "}\n",
        "\n",
        "class NewsCollector:\n",
        "    def __init__(self, feeds_dict, days_limit=14):\n",
        "        \"\"\"\n",
        "        Initialize the NewsCollector with RSS feeds and time limit\n",
        "\n",
        "        Args:\n",
        "            feeds_dict: Dictionary of RSS feeds with name as key and URL as value\n",
        "            days_limit: Number of days to look back for articles\n",
        "        \"\"\"\n",
        "        self.feeds_dict = feeds_dict\n",
        "        self.days_limit = days_limit\n",
        "        self.articles_df = None\n",
        "\n",
        "    def fetch_articles(self):\n",
        "        \"\"\"Fetch articles from all feeds and filter by date\"\"\"\n",
        "        all_articles = []\n",
        "        cutoff_date = datetime.now() - timedelta(days=self.days_limit)\n",
        "\n",
        "        for source_name, feed_url in self.feeds_dict.items():\n",
        "            try:\n",
        "                print(f\"Fetching articles from {source_name}...\")\n",
        "                feed = feedparser.parse(feed_url)\n",
        "\n",
        "                for entry in feed.entries:\n",
        "                    # Handle different date formats\n",
        "                    if hasattr(entry, 'published_parsed'):\n",
        "                        pub_date = datetime(*entry.published_parsed[:6])\n",
        "                    elif hasattr(entry, 'updated_parsed'):\n",
        "                        pub_date = datetime(*entry.updated_parsed[:6])\n",
        "                    else:\n",
        "                        # Skip if no date available\n",
        "                        continue\n",
        "\n",
        "                    # Filter by date\n",
        "                    if pub_date >= cutoff_date:\n",
        "                        article = {\n",
        "                            'title': entry.title,\n",
        "                            'summary': self._clean_html(entry.summary if hasattr(entry, 'summary') else \"\"),\n",
        "                            'link': entry.link,\n",
        "                            'published_date': pub_date,\n",
        "                            'source': source_name\n",
        "                        }\n",
        "                        all_articles.append(article)\n",
        "\n",
        "                # Be nice to the servers\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching {source_name}: {str(e)}\")\n",
        "\n",
        "        # Convert to dataframe\n",
        "        self.articles_df = pd.DataFrame(all_articles)\n",
        "        print(f\"Collected {len(self.articles_df)} articles\")\n",
        "        return self.articles_df\n",
        "\n",
        "    def _clean_html(self, text):\n",
        "        \"\"\"Remove HTML tags from text\"\"\"\n",
        "        return BeautifulSoup(text, 'html.parser').get_text()\n",
        "\n",
        "    def save_articles(self, filename=\"collected_news_articles.csv\"):\n",
        "        \"\"\"Save articles to CSV file\"\"\"\n",
        "        if self.articles_df is not None:\n",
        "            self.articles_df.to_csv(filename, index=False)\n",
        "            print(f\"Articles saved to {filename}\")\n",
        "        else:\n",
        "            print(\"No articles to save. Run fetch_articles() first.\")\n",
        "\n",
        "    def load_articles(self, filename=\"collected_news_articles.csv\"):\n",
        "        \"\"\"Load articles from CSV file\"\"\"\n",
        "        self.articles_df = pd.read_csv(filename)\n",
        "        # Convert date string back to datetime\n",
        "        self.articles_df['published_date'] = pd.to_datetime(self.articles_df['published_date'])\n",
        "        print(f\"Loaded {len(self.articles_df)} articles from {filename}\")\n",
        "        return self.articles_df\n",
        "\n",
        "class NLPProcessor:\n",
        "    def __init__(self, model):\n",
        "        \"\"\"\n",
        "        Initialize the NLP processor with a Gemini model\n",
        "\n",
        "        Args:\n",
        "            model: Initialized Gemini model\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.processed_data = []\n",
        "\n",
        "    def extract_entities_events(self, text):\n",
        "        \"\"\"\n",
        "        Extract named entities, events, and sentiment using Gemini\n",
        "\n",
        "        Args:\n",
        "            text: Article text (title + summary)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with extracted entities, events, and sentiment\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Analyze this news article text and extract the following information in JSON format:\n",
        "\n",
        "        Text: {text}\n",
        "\n",
        "        Return a JSON object with these keys:\n",
        "        1. \"entities\": Extract named entities as an object with these categories as keys:\n",
        "           - \"organizations\": List of organization names\n",
        "           - \"persons\": List of person names\n",
        "           - \"locations\": List of location names\n",
        "           - \"products\": List of product names (if any)\n",
        "           - \"events\": List of event types/names (if any)\n",
        "\n",
        "        2. \"key_events\": List of main events or actions described (verb-focused phrases, 2-5 items)\n",
        "\n",
        "        3. \"sentiment\": Object with:\n",
        "           - \"overall\": Overall sentiment of the article (\"positive\", \"negative\", or \"neutral\")\n",
        "           - \"entity_sentiments\": Array of objects with \"entity\" and \"sentiment\" for main entities\n",
        "\n",
        "        Return ONLY the JSON with no additional text.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            # Extract JSON from response\n",
        "            response_text = response.text\n",
        "\n",
        "            # Handle cases where the model might include markdown code blocks\n",
        "            if \"```json\" in response_text:\n",
        "                json_text = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "            elif \"```\" in response_text:\n",
        "                json_text = response_text.split(\"```\")[1].strip()\n",
        "            else:\n",
        "                json_text = response_text.strip()\n",
        "\n",
        "            extracted_data = json.loads(json_text)\n",
        "            return extracted_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in NLP processing: {str(e)}\")\n",
        "            # Return empty structure if analysis fails\n",
        "            return {\n",
        "                \"entities\": {\n",
        "                    \"organizations\": [],\n",
        "                    \"persons\": [],\n",
        "                    \"locations\": [],\n",
        "                    \"products\": [],\n",
        "                    \"events\": []\n",
        "                },\n",
        "                \"key_events\": [],\n",
        "                \"sentiment\": {\n",
        "                    \"overall\": \"neutral\",\n",
        "                    \"entity_sentiments\": []\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def process_articles(self, articles_df):\n",
        "        \"\"\"\n",
        "        Process all articles in the dataframe\n",
        "\n",
        "        Args:\n",
        "            articles_df: DataFrame with articles\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with processed articles\n",
        "        \"\"\"\n",
        "        self.processed_data = []\n",
        "\n",
        "        for _, article in articles_df.iterrows():\n",
        "            # Combine title and summary for better context\n",
        "            full_text = f\"{article['title']} {article['summary']}\"\n",
        "\n",
        "            print(f\"Processing article: {article['title'][:40]}...\")\n",
        "\n",
        "            try:\n",
        "                # Extract information using NLP\n",
        "                extracted_info = self.extract_entities_events(full_text)\n",
        "\n",
        "                # Store processed data\n",
        "                processed_article = {\n",
        "                    'article_id': _,\n",
        "                    'title': article['title'],\n",
        "                    'source': article['source'],\n",
        "                    'published_date': article['published_date'],\n",
        "                    'link': article['link'],\n",
        "                    'extracted_info': extracted_info\n",
        "                }\n",
        "\n",
        "                self.processed_data.append(processed_article)\n",
        "\n",
        "                # Be nice to the API rate limits\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing article: {str(e)}\")\n",
        "\n",
        "        print(f\"Processed {len(self.processed_data)} articles\")\n",
        "        return self.processed_data\n",
        "\n",
        "    def save_processed_data(self, filename=\"processed_articles.json\"):\n",
        "        \"\"\"Save processed data to JSON file\"\"\"\n",
        "        with open(filename, 'w') as f:\n",
        "            # Convert datetime objects to strings for JSON serialization\n",
        "            serializable_data = []\n",
        "            for item in self.processed_data:\n",
        "                serialized_item = item.copy()\n",
        "                if isinstance(serialized_item['published_date'], pd.Timestamp):\n",
        "                    serialized_item['published_date'] = serialized_item['published_date'].isoformat()\n",
        "                serializable_data.append(serialized_item)\n",
        "\n",
        "            json.dump(serializable_data, f, indent=2)\n",
        "        print(f\"Processed data saved to {filename}\")\n",
        "\n",
        "    def load_processed_data(self, filename=\"processed_articles.json\"):\n",
        "        \"\"\"Load processed data from JSON file\"\"\"\n",
        "        with open(filename, 'r') as f:\n",
        "            self.processed_data = json.load(f)\n",
        "        print(f\"Loaded {len(self.processed_data)} processed articles from {filename}\")\n",
        "        return self.processed_data\n",
        "\n",
        "class KnowledgeGraphBuilder:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the knowledge graph builder\"\"\"\n",
        "        self.G = nx.Graph()\n",
        "        self.entity_types = {\n",
        "            \"organizations\": \"Organization\",\n",
        "            \"persons\": \"Person\",\n",
        "            \"locations\": \"Location\",\n",
        "            \"products\": \"Product\",\n",
        "            \"events\": \"Event\"\n",
        "        }\n",
        "        self.sentiment_colors = {\n",
        "            \"positive\": \"green\",\n",
        "            \"neutral\": \"gray\",\n",
        "            \"negative\": \"red\"\n",
        "        }\n",
        "\n",
        "    def build_graph(self, processed_data):\n",
        "        \"\"\"\n",
        "        Build knowledge graph from processed article data\n",
        "\n",
        "        Args:\n",
        "            processed_data: List of processed articles with extracted information\n",
        "        \"\"\"\n",
        "        # Reset graph\n",
        "        self.G = nx.Graph()\n",
        "\n",
        "        for article in processed_data:\n",
        "            article_id = article['article_id']\n",
        "            source = article['source']\n",
        "            date = article['published_date']\n",
        "            link = article['link']\n",
        "            title = article['title']\n",
        "\n",
        "            # Add article node\n",
        "            self.G.add_node(\n",
        "                f\"article_{article_id}\",\n",
        "                type=\"Article\",\n",
        "                title=title,\n",
        "                source=source,\n",
        "                date=date,\n",
        "                link=link,\n",
        "                sentiment=article['extracted_info']['sentiment']['overall']\n",
        "            )\n",
        "\n",
        "            # Add entity nodes and connect to article\n",
        "            for entity_type, entities in article['extracted_info']['entities'].items():\n",
        "                if entity_type in self.entity_types and entities:\n",
        "                    for entity in entities:\n",
        "                        entity_id = f\"{entity.lower().replace(' ', '_')}_{self.entity_types[entity_type]}\"\n",
        "\n",
        "                        # Add entity if not exists\n",
        "                        if not self.G.has_node(entity_id):\n",
        "                            self.G.add_node(\n",
        "                                entity_id,\n",
        "                                name=entity,\n",
        "                                type=self.entity_types[entity_type]\n",
        "                            )\n",
        "\n",
        "                        # Connect entity to article\n",
        "                        self.G.add_edge(\n",
        "                            entity_id,\n",
        "                            f\"article_{article_id}\",\n",
        "                            type=\"mentioned_in\"\n",
        "                        )\n",
        "\n",
        "            # Add connections between entities in the same article\n",
        "            entity_ids = []\n",
        "            for entity_type, entities in article['extracted_info']['entities'].items():\n",
        "                if entity_type in self.entity_types and entities:\n",
        "                    for entity in entities:\n",
        "                        entity_id = f\"{entity.lower().replace(' ', '_')}_{self.entity_types[entity_type]}\"\n",
        "                        entity_ids.append(entity_id)\n",
        "\n",
        "            # Connect co-occurring entities\n",
        "            for i, entity1 in enumerate(entity_ids):\n",
        "                for entity2 in entity_ids[i+1:]:\n",
        "                    if self.G.has_edge(entity1, entity2):\n",
        "                        # Increment weight if edge exists\n",
        "                        self.G[entity1][entity2]['weight'] += 1\n",
        "                    else:\n",
        "                        # Create new edge\n",
        "                        self.G.add_edge(entity1, entity2, type=\"co_occurs_with\", weight=1)\n",
        "\n",
        "            # Add sentiment information to entities\n",
        "            for entity_sentiment in article['extracted_info']['sentiment']['entity_sentiments']:\n",
        "                entity = entity_sentiment.get('entity')\n",
        "                sentiment = entity_sentiment.get('sentiment')\n",
        "\n",
        "                if entity and sentiment:\n",
        "                    # Try to find the entity in different types\n",
        "                    for entity_type in self.entity_types.values():\n",
        "                        entity_id = f\"{entity.lower().replace(' ', '_')}_{entity_type}\"\n",
        "                        if self.G.has_node(entity_id):\n",
        "                            # Add or update sentiment attribute as a string\n",
        "                            if 'sentiments_str' not in self.G.nodes[entity_id]:\n",
        "                                self.G.nodes[entity_id]['sentiments_str'] = sentiment\n",
        "                            else:\n",
        "                                self.G.nodes[entity_id]['sentiments_str'] += \",\" + sentiment\n",
        "\n",
        "                            # Keep track of predominant sentiment\n",
        "                            sentiments_list = self.G.nodes[entity_id]['sentiments_str'].split(',')\n",
        "                            pos_count = sentiments_list.count('positive')\n",
        "                            neg_count = sentiments_list.count('negative')\n",
        "                            neu_count = sentiments_list.count('neutral')\n",
        "\n",
        "                            if pos_count >= neg_count and pos_count >= neu_count:\n",
        "                                self.G.nodes[entity_id]['predominant_sentiment'] = 'positive'\n",
        "                            elif neg_count >= pos_count and neg_count >= neu_count:\n",
        "                                self.G.nodes[entity_id]['predominant_sentiment'] = 'negative'\n",
        "                            else:\n",
        "                                self.G.nodes[entity_id]['predominant_sentiment'] = 'neutral'\n",
        "                            break\n",
        "\n",
        "        print(f\"Built graph with {self.G.number_of_nodes()} nodes and {self.G.number_of_edges()} edges\")\n",
        "        return self.G\n",
        "\n",
        "    def save_graph(self, filename=\"news_knowledge_graph.graphml\"):\n",
        "        \"\"\"Save graph to GraphML file\"\"\"\n",
        "        # Convert non-serializable attributes\n",
        "        for node, data in self.G.nodes(data=True):\n",
        "            for key, value in data.items():\n",
        "                if isinstance(value, (pd.Timestamp, datetime)):\n",
        "                    self.G.nodes[node][key] = str(value)\n",
        "                # Convert any remaining lists to strings to avoid GraphML errors\n",
        "                elif isinstance(value, list):\n",
        "                    self.G.nodes[node][key] = ','.join(map(str, value))\n",
        "\n",
        "        # Also check edge attributes\n",
        "        for u, v, data in self.G.edges(data=True):\n",
        "            for key, value in list(data.items()):\n",
        "                if isinstance(value, list):\n",
        "                    data[key] = ','.join(map(str, value))\n",
        "\n",
        "        # Save graph\n",
        "        nx.write_graphml(self.G, filename)\n",
        "        print(f\"Graph saved to {filename}\")\n",
        "\n",
        "        # Also save as GEXF for Gephi compatibility\n",
        "        gexf_file = filename.replace('.graphml', '.gexf')\n",
        "        nx.write_gexf(self.G, gexf_file)\n",
        "        print(f\"Graph also saved to {gexf_file}\")\n",
        "\n",
        "    def load_graph(self, filename=\"news_knowledge_graph.graphml\"):\n",
        "        \"\"\"Load graph from GraphML file\"\"\"\n",
        "        self.G = nx.read_graphml(filename)\n",
        "        print(f\"Loaded graph with {self.G.number_of_nodes()} nodes and {self.G.number_of_edges()} edges\")\n",
        "        return self.G\n",
        "\n",
        "class RiskAnalyzer:\n",
        "    def __init__(self, graph):\n",
        "        \"\"\"\n",
        "        Initialize the risk analyzer\n",
        "\n",
        "        Args:\n",
        "            graph: NetworkX graph with news entities and articles\n",
        "        \"\"\"\n",
        "        self.G = graph\n",
        "        self.risk_signals = []\n",
        "\n",
        "    def calculate_node_metrics(self):\n",
        "        \"\"\"Calculate centrality measures for all nodes\"\"\"\n",
        "        # Degree centrality\n",
        "        degree_cent = nx.degree_centrality(self.G)\n",
        "        # Betweenness centrality (can be computationally expensive for large graphs)\n",
        "        betweenness_cent = nx.betweenness_centrality(self.G, k=10)  # Use sampling for larger graphs\n",
        "\n",
        "        # Add metrics to graph\n",
        "        for node in self.G.nodes():\n",
        "            self.G.nodes[node]['degree_centrality'] = degree_cent[node]\n",
        "            self.G.nodes[node]['betweenness_centrality'] = betweenness_cent[node]\n",
        "\n",
        "        return {\n",
        "            'degree_centrality': degree_cent,\n",
        "            'betweenness_centrality': betweenness_cent\n",
        "        }\n",
        "\n",
        "    def find_communities(self):\n",
        "        \"\"\"Find communities in the graph using Louvain method\"\"\"\n",
        "        communities = community.louvain_communities(self.G)\n",
        "\n",
        "        # Add community membership to nodes\n",
        "        for i, comm in enumerate(communities):\n",
        "            for node in comm:\n",
        "                self.G.nodes[node]['community'] = i\n",
        "\n",
        "        return communities\n",
        "\n",
        "    def identify_risk_signals(self):\n",
        "        \"\"\"\n",
        "        Identify potential risk signals in the graph\n",
        "\n",
        "        Returns:\n",
        "            List of risk signals with details\n",
        "        \"\"\"\n",
        "        self.risk_signals = []\n",
        "\n",
        "        # 1. Identify highly connected entities involved in negative sentiment articles\n",
        "        for node in self.G.nodes():\n",
        "            if not node.startswith('article_') and self.G.nodes[node].get('type') != 'Article':\n",
        "\n",
        "                # Check if entity appears in multiple articles\n",
        "                connected_articles = [n for n in self.G.neighbors(node) if n.startswith('article_')]\n",
        "\n",
        "                if len(connected_articles) >= 2:  # Entity appears in multiple articles\n",
        "\n",
        "                    # Count negative sentiment articles\n",
        "                    negative_articles = [\n",
        "                        art for art in connected_articles\n",
        "                        if self.G.nodes[art].get('sentiment') == 'negative'\n",
        "                    ]\n",
        "\n",
        "                    if len(negative_articles) >= 1:\n",
        "                        risk_signal = {\n",
        "                            'entity': node,\n",
        "                            'entity_name': self.G.nodes[node].get('name', node),\n",
        "                            'entity_type': self.G.nodes[node].get('type', 'Unknown'),\n",
        "                            'degree_centrality': self.G.nodes[node].get('degree_centrality', 0),\n",
        "                            'connected_articles': len(connected_articles),\n",
        "                            'negative_articles': len(negative_articles),\n",
        "                            'negative_ratio': len(negative_articles) / len(connected_articles),\n",
        "                            'risk_type': 'High-visibility entity with negative sentiment'\n",
        "                        }\n",
        "                        self.risk_signals.append(risk_signal)\n",
        "\n",
        "        # 2. Find clusters of entities that co-occur often with negative sentiment\n",
        "        communities = self.find_communities()\n",
        "\n",
        "        for i, comm in enumerate(communities):\n",
        "            # Filter for non-article nodes\n",
        "            entity_nodes = [n for n in comm if not n.startswith('article_')]\n",
        "\n",
        "            if len(entity_nodes) >= 3:  # Only consider substantial communities\n",
        "                # Find all articles connected to this community\n",
        "                community_articles = set()\n",
        "                for entity in entity_nodes:\n",
        "                    for neighbor in self.G.neighbors(entity):\n",
        "                        if neighbor.startswith('article_'):\n",
        "                            community_articles.add(neighbor)\n",
        "\n",
        "                # Count negative sentiment articles\n",
        "                negative_articles = [\n",
        "                    art for art in community_articles\n",
        "                    if self.G.nodes[art].get('sentiment') == 'negative'\n",
        "                ]\n",
        "\n",
        "                if len(negative_articles) >= 2 and len(negative_articles)/len(community_articles) >= 0.3:\n",
        "                    risk_signal = {\n",
        "                        'community_id': i,\n",
        "                        'entity_count': len(entity_nodes),\n",
        "                        'entities': [self.G.nodes[n].get('name', n) for n in entity_nodes[:5]],  # Top 5 entities\n",
        "                        'connected_articles': len(community_articles),\n",
        "                        'negative_articles': len(negative_articles),\n",
        "                        'negative_ratio': len(negative_articles) / len(community_articles),\n",
        "                        'risk_type': 'Entity cluster with negative sentiment'\n",
        "                    }\n",
        "                    self.risk_signals.append(risk_signal)\n",
        "\n",
        "        print(f\"Identified {len(self.risk_signals)} potential risk signals\")\n",
        "        return self.risk_signals\n",
        "\n",
        "    def extract_risk_subgraph(self, risk_signal, max_nodes=20):\n",
        "        \"\"\"\n",
        "        Extract a subgraph related to a specific risk signal\n",
        "\n",
        "        Args:\n",
        "            risk_signal: Risk signal dictionary\n",
        "            max_nodes: Maximum number of nodes in the subgraph\n",
        "\n",
        "        Returns:\n",
        "            NetworkX subgraph\n",
        "        \"\"\"\n",
        "        if 'entity' in risk_signal:\n",
        "            # Case 1: Risk based on a specific entity\n",
        "            entity = risk_signal['entity']\n",
        "            nodes = set([entity])\n",
        "\n",
        "            # Add connected articles\n",
        "            connected_articles = [n for n in self.G.neighbors(entity) if n.startswith('article_')]\n",
        "            nodes.update(connected_articles)\n",
        "\n",
        "            # Add other entities connected to these articles (limited to maintain clarity)\n",
        "            for article in connected_articles[:min(5, len(connected_articles))]:\n",
        "                for neighbor in self.G.neighbors(article):\n",
        "                    if not neighbor.startswith('article_') and len(nodes) < max_nodes:\n",
        "                        nodes.add(neighbor)\n",
        "\n",
        "        elif 'community_id' in risk_signal:\n",
        "            # Case 2: Risk based on a community\n",
        "            community_id = risk_signal['community_id']\n",
        "            nodes = set()\n",
        "\n",
        "            # Get entities in this community\n",
        "            for node, data in self.G.nodes(data=True):\n",
        "                if data.get('community') == community_id and not node.startswith('article_'):\n",
        "                    nodes.add(node)\n",
        "                    if len(nodes) >= max_nodes * 0.5:  # Limit to half the max nodes\n",
        "                        break\n",
        "\n",
        "            # Add connected articles and their entities\n",
        "            articles = set()\n",
        "            for entity in list(nodes):\n",
        "                for neighbor in self.G.neighbors(entity):\n",
        "                    if neighbor.startswith('article_'):\n",
        "                        articles.add(neighbor)\n",
        "                        if len(articles) >= 5:  # Limit to 5 articles\n",
        "                            break\n",
        "\n",
        "            nodes.update(articles)\n",
        "\n",
        "            # Add some more entities from these articles\n",
        "            for article in articles:\n",
        "                for neighbor in self.G.neighbors(article):\n",
        "                    if not neighbor.startswith('article_') and len(nodes) < max_nodes:\n",
        "                        nodes.add(neighbor)\n",
        "\n",
        "        # Create subgraph\n",
        "        subgraph = self.G.subgraph(nodes).copy()\n",
        "        return subgraph\n",
        "\n",
        "    def visualize_risk_subgraph(self, subgraph, title=\"Risk Signal Subgraph\", filename=\"risk_subgraph.png\"):\n",
        "        \"\"\"\n",
        "        Visualize a risk subgraph\n",
        "\n",
        "        Args:\n",
        "            subgraph: NetworkX subgraph\n",
        "            title: Plot title\n",
        "            filename: Output filename\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # Create position layout\n",
        "        pos = nx.spring_layout(subgraph, seed=42)\n",
        "\n",
        "        # Prepare node colors based on type\n",
        "        node_colors = []\n",
        "        node_sizes = []\n",
        "\n",
        "        for node in subgraph.nodes():\n",
        "            if node.startswith('article_'):\n",
        "                # Color articles by sentiment\n",
        "                sentiment = subgraph.nodes[node].get('sentiment', 'neutral')\n",
        "                if sentiment == 'positive':\n",
        "                    node_colors.append('green')\n",
        "                elif sentiment == 'negative':\n",
        "                    node_colors.append('red')\n",
        "                else:\n",
        "                    node_colors.append('gray')\n",
        "                node_sizes.append(600)\n",
        "            else:\n",
        "                # Color entities by type\n",
        "                entity_type = subgraph.nodes[node].get('type', 'Unknown')\n",
        "                if entity_type == 'Organization':\n",
        "                    node_colors.append('blue')\n",
        "                elif entity_type == 'Person':\n",
        "                    node_colors.append('orange')\n",
        "                elif entity_type == 'Location':\n",
        "                    node_colors.append('purple')\n",
        "                elif entity_type == 'Product':\n",
        "                    node_colors.append('cyan')\n",
        "                elif entity_type == 'Event':\n",
        "                    node_colors.append('magenta')\n",
        "                else:\n",
        "                    node_colors.append('yellow')\n",
        "                node_sizes.append(300)\n",
        "\n",
        "        # Draw nodes\n",
        "        nx.draw_networkx_nodes(\n",
        "            subgraph, pos,\n",
        "            node_color=node_colors,\n",
        "            node_size=node_sizes,\n",
        "            alpha=0.8\n",
        "        )\n",
        "\n",
        "        # Draw edges\n",
        "        nx.draw_networkx_edges(\n",
        "            subgraph, pos,\n",
        "            edge_color='gray',\n",
        "            width=1.0,\n",
        "            alpha=0.5\n",
        "        )\n",
        "\n",
        "        # Prepare labels\n",
        "        labels = {}\n",
        "        for node in subgraph.nodes():\n",
        "            if node.startswith('article_'):\n",
        "                # Truncate article titles for readability\n",
        "                title = subgraph.nodes[node].get('title', '')\n",
        "                labels[node] = title[:20] + '...' if len(title) > 20 else title\n",
        "            else:\n",
        "                # Use entity names\n",
        "                labels[node] = subgraph.nodes[node].get('name', node.split('_')[0])\n",
        "\n",
        "        # Draw labels\n",
        "        nx.draw_networkx_labels(\n",
        "            subgraph, pos,\n",
        "            labels=labels,\n",
        "            font_size=8,\n",
        "            font_color='black'\n",
        "        )\n",
        "\n",
        "        # Create legend\n",
        "        legend_elements = [\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Negative Article'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Positive Article'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=10, label='Neutral Article'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Organization'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Person'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='purple', markersize=10, label='Location'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='cyan', markersize=10, label='Product'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='magenta', markersize=10, label='Event')\n",
        "        ]\n",
        "        plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Visualization saved to {filename}\")\n",
        "\n",
        "    def generate_risk_report(self, top_n=5, filename=\"risk_analysis_report.json\"):\n",
        "        \"\"\"\n",
        "        Generate a comprehensive risk report based on the analysis\n",
        "\n",
        "        Args:\n",
        "            top_n: Number of top risk signals to include in the report\n",
        "            filename: Output filename for the report JSON\n",
        "        \"\"\"\n",
        "        # Sort risk signals by negative_ratio * connected_articles (impact metric)\n",
        "        for signal in self.risk_signals:\n",
        "            signal['impact_score'] = signal.get('negative_ratio', 0) * signal.get('connected_articles', 0)\n",
        "\n",
        "        sorted_signals = sorted(self.risk_signals, key=lambda x: x.get('impact_score', 0), reverse=True)\n",
        "        top_signals = sorted_signals[:top_n]\n",
        "\n",
        "        # Generate visualizations for top signals\n",
        "        for i, signal in enumerate(top_signals):\n",
        "            try:\n",
        "                # Extract and visualize subgraph\n",
        "                subgraph = self.extract_risk_subgraph(signal)\n",
        "\n",
        "                # Generate title based on signal type\n",
        "                if 'entity' in signal:\n",
        "                    title = f\"Risk Signal: {signal['entity_name']} ({signal['entity_type']})\"\n",
        "                else:\n",
        "                    title = f\"Risk Signal: Community Cluster {signal['community_id']}\"\n",
        "\n",
        "                vis_filename = f\"risk_signal_{i+1}.png\"\n",
        "                self.visualize_risk_subgraph(subgraph, title=title, filename=vis_filename)\n",
        "\n",
        "                # Add visualization filename to the signal\n",
        "                signal['visualization'] = vis_filename\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating visualization for risk signal {i+1}: {str(e)}\")\n",
        "\n",
        "        # Compile report\n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'total_risk_signals': len(self.risk_signals),\n",
        "            'top_risk_signals': top_signals,\n",
        "            'graph_metrics': {\n",
        "                'total_nodes': self.G.number_of_nodes(),\n",
        "                'total_edges': self.G.number_of_edges(),\n",
        "                'density': nx.density(self.G),\n",
        "                'average_clustering': nx.average_clustering(self.G),\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save report\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(report, f, indent=2, default=str)\n",
        "\n",
        "        print(f\"Risk analysis report saved to {filename}\")\n",
        "        return report\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the news analysis system\"\"\"\n",
        "    print(\"Starting News Analysis System...\")\n",
        "\n",
        "    # Step 1: Collect news articles\n",
        "    collector = NewsCollector(rss_feeds, days_limit=14)\n",
        "\n",
        "    try:\n",
        "        # Try to load existing articles first\n",
        "        articles_df = collector.load_articles()\n",
        "    except:\n",
        "        # If file doesn't exist, fetch new articles\n",
        "        articles_df = collector.fetch_articles()\n",
        "        collector.save_articles()\n",
        "\n",
        "    # Step 2: Process articles with NLP\n",
        "    processor = NLPProcessor(model)\n",
        "\n",
        "    try:\n",
        "        # Try to load existing processed data\n",
        "        processed_data = processor.load_processed_data()\n",
        "    except:\n",
        "        # If file doesn't exist, process articles\n",
        "        processed_data = processor.process_articles(articles_df)\n",
        "        processor.save_processed_data()\n",
        "\n",
        "    # Step 3: Build knowledge graph\n",
        "    graph_builder = KnowledgeGraphBuilder()\n",
        "    graph = graph_builder.build_graph(processed_data)\n",
        "    graph_builder.save_graph()\n",
        "\n",
        "    # Step 4: Analyze graph for risk signals\n",
        "    risk_analyzer = RiskAnalyzer(graph)\n",
        "    risk_analyzer.calculate_node_metrics()\n",
        "    risk_signals = risk_analyzer.identify_risk_signals()\n",
        "    risk_report = risk_analyzer.generate_risk_report()\n",
        "\n",
        "    print(\"\\nAnalysis complete!\")\n",
        "    print(f\"Found {len(risk_signals)} potential risk signals\")\n",
        "    print(\"Check the output files for detailed results.\")\n",
        "\n",
        "    # Print summary of top risk signals\n",
        "    print(\"\\nTop Risk Signals Summary:\")\n",
        "    for i, signal in enumerate(risk_report['top_risk_signals'][:3]):\n",
        "        print(f\"\\n{i+1}. \", end=\"\")\n",
        "        if 'entity' in signal:\n",
        "            print(f\"Entity: {signal['entity_name']} ({signal['entity_type']})\")\n",
        "            print(f\"   Connected to {signal['connected_articles']} articles ({signal['negative_articles']} negative)\")\n",
        "            print(f\"   Impact score: {signal['impact_score']:.2f}\")\n",
        "        else:\n",
        "            print(f\"Community cluster with {signal['entity_count']} entities\")\n",
        "            print(f\"   Key entities: {', '.join(signal['entities'][:3])}\")\n",
        "            print(f\"   Connected to {signal['connected_articles']} articles ({signal['negative_articles']} negative)\")\n",
        "            print(f\"   Impact score: {signal['impact_score']:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1XVa1lhbtZlA",
        "outputId": "e19cb67e-e566-460e-a50b-efdc3ba3484f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting News Analysis System...\n",
            "Fetching articles from Reuters Business...\n",
            "Fetching articles from BBC World...\n",
            "Fetching articles from Al Jazeera...\n",
            "Collected 47 articles\n",
            "Articles saved to collected_news_articles.csv\n",
            "Processing article: European leaders pressure Russia over 30...\n",
            "Processing article: Trump administration considers suspendin...\n",
            "Processing article: Maga says Pope Leo may be American, but ...\n",
            "Processing article: Taylor Swift criticises Lively-Baldoni c...\n",
            "Processing article: How will Pope Leo lead? His first days m...\n",
            "Processing article: Elton John and Dua Lipa seek protection ...\n",
            "Processing article: Mexico sues Google over 'Gulf of America...\n",
            "Processing article: Turkish Tufts University student release...\n",
            "Processing article: US confirms plan for private firms to de...\n",
            "Processing article: Xi shows he wants to be close to Putin -...\n",
            "Processing article: No water, no power - Port Sudan reeling ...\n",
            "Processing article: Moon dust 'rarer than gold' arrives in U...\n",
            "Processing article: 'I freaked out and spent $400 online': U...\n",
            "Processing article: Witchcraft, innuendo and moody goth boys...\n",
            "Processing article: The US and China are finally talking. Wh...\n",
            "Processing article: How can India and Pakistan find a way to...\n",
            "Processing article: Hong Kong pro-China informer: 'Why I've ...\n",
            "Processing article: White House 'actively looking at' suspen...\n",
            "Processing article: Watch: Steve Rosenberg reports from Russ...\n",
            "Processing article: Watch: Trump misspeaks and calls toy fir...\n",
            "Processing article: Three things we learned from Putin's Red...\n",
            "Processing article: Watch: Pope Leo XIV's first Mass as pont...\n",
            "Processing article: Iran’s FM visits Saudi Arabia, Qatar bef...\n",
            "Processing article: Pope Leo identifies AI as main challenge...\n",
            "Processing article: Liverpool vs Arsenal: Premier League – S...\n",
            "Processing article: China, US hold talks on tariffs in first...\n",
            "Processing article: My nephew asks if he will eat meat only ...\n",
            "Processing article: US to fast-track investments from Middle...\n",
            "Processing article: Children among 13 killed in Israel’s att...\n",
            "Processing article: Qatar leads Syria search for bodies of U...\n",
            "Processing article: Iraq look to former Australia coach Arno...\n",
            "Processing article: Pakistan launches Operation Bunyan Marso...\n",
            "Processing article: Pakistan postpones PSL T20 cricket amid ...\n",
            "Processing article: Nuggets outlast Thunder in Game 3 overti...\n",
            "Processing article: Russia-Ukraine war: List of key events, ...\n",
            "Processing article: European leaders in Ukraine’s Kyiv press...\n",
            "Processing article: Netanyahu’s war choices fuel discord in ...\n",
            "Processing article: After Israel’s bombs, Nabatieh’s Monday ...\n",
            "Processing article: Reporting from behind shifting front lin...\n",
            "Processing article: Could India, Pakistan use nuclear weapon...\n",
            "Processing article: Newark Mayor Ras Baraka arrested during ...\n",
            "Processing article: World could be witnessing ‘another Nakba...\n",
            "Processing article: US reports second air traffic control ou...\n",
            "Processing article: India, Pakistan exchange claims over dro...\n",
            "Processing article: Mexico is suing Google over ‘Gulf of Ame...\n",
            "Processing article: Barcelona vs Real Madrid: El Clasico La ...\n",
            "Processing article: Mohsen Mahdawi tells Al Jazeera why he r...\n",
            "Processed 47 articles\n",
            "Processed data saved to processed_articles.json\n",
            "Built graph with 206 nodes and 651 edges\n",
            "Graph saved to news_knowledge_graph.graphml\n",
            "Graph also saved to news_knowledge_graph.gexf\n",
            "Identified 27 potential risk signals\n",
            "Visualization saved to risk_signal_1.png\n",
            "Visualization saved to risk_signal_2.png\n",
            "Visualization saved to risk_signal_3.png\n",
            "Visualization saved to risk_signal_4.png\n",
            "Visualization saved to risk_signal_5.png\n",
            "Risk analysis report saved to risk_analysis_report.json\n",
            "\n",
            "Analysis complete!\n",
            "Found 27 potential risk signals\n",
            "Check the output files for detailed results.\n",
            "\n",
            "Top Risk Signals Summary:\n",
            "\n",
            "1. Community cluster with 17 entities\n",
            "   Key entities: ban on entry of all items, bombardments, UN\n",
            "   Connected to 7 articles (6 negative)\n",
            "   Impact score: 6.00\n",
            "\n",
            "2. Community cluster with 10 entities\n",
            "   Key entities: Twenty20 league, war, nuclear weapons\n",
            "   Connected to 6 articles (6 negative)\n",
            "   Impact score: 6.00\n",
            "\n",
            "3. Community cluster with 24 entities\n",
            "   Key entities: air traffic control outage, Sheinbaum, New Jersey\n",
            "   Connected to 11 articles (5 negative)\n",
            "   Impact score: 5.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QRyU2dEgtZqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZfn9hB0tZuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bQc8lKNItZxg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}